{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_OASIS():\n",
    "    with h5py.File('..\\..\\Datasets\\OASIS_balanced.h5', 'r') as hdf:\n",
    "\n",
    "        G1 = hdf.get('Train Data')\n",
    "        trainX = np.array(G1.get('trainX'))\n",
    "        trainY = np.array(G1.get('trainY'))\n",
    "        G2 = hdf.get('Test Data')\n",
    "        testX = np.array(G2.get('testX'))\n",
    "        testY = np.array(G2.get('testY'))\n",
    "\n",
    "        return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ADNI():\n",
    "    with h5py.File('..\\..\\Datasets\\ADNI_enhanced.h5', 'r') as hdf:\n",
    "\n",
    "        G1 = hdf.get('Train Data')\n",
    "        trainX = np.array(G1.get('x_train'))\n",
    "        trainY = np.array(G1.get('y_train'))\n",
    "        G2 = hdf.get('Test Data')\n",
    "        testX = np.array(G2.get('x_test'))\n",
    "        testY = np.array(G2.get('y_test'))\n",
    "\n",
    "        return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8192, 176, 176) (8192,) (2560, 176, 176) (2560,)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'ADNdI'\n",
    "#dataset = 'ADsNI'\n",
    "# read the data which is also normalized.\n",
    "if dataset == 'ADNI':\n",
    "    x_train, y_train, x_test, y_test = load_ADNI()\n",
    "else:\n",
    "    x_train, y_train, x_test, y_test = load_OASIS()\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test (2560, 176, 176, 3)\n",
      "Train (8192, 176, 176, 3)\n",
      "(8192,)\n",
      "(8192, 4)\n",
      "(2560, 4)\n",
      "176 176\n"
     ]
    }
   ],
   "source": [
    "#Dataset ready for deep learning\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_train = np.repeat(x_train, 3, axis=3)\n",
    "\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "x_test = np.repeat(x_test, 3, axis=3)\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=4)\n",
    "y_test_cat = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "print(\"Test\",x_test.shape)\n",
    "print (\"Train\",x_train.shape)\n",
    "print(y_train.shape)\n",
    "print (y_train_cat.shape)\n",
    "print (y_test_cat.shape)\n",
    "row = x_train.shape[1]\n",
    "col = x_test.shape[2]\n",
    "print(row,col)\n",
    "#print(np.max(x_train[0])) #to ensure that the values are within the range [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2560, 176, 176, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print()\n",
    "epochs = 10 #hyperparameter\n",
    "num_classes=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Functional)    (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_700 (Bat (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "batch_normalization_701 (Bat (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "activation_696 (Activation)  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_702 (Bat (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_697 (Activation)  (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 28,121,892\n",
      "Trainable params: 6,308,868\n",
      "Non-trainable params: 21,813,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, MaxPooling2D, Activation, BatchNormalization\n",
    "import tensorflow as tf\n",
    "model1 = Sequential()\n",
    "\n",
    "\n",
    "pretrained_model= tf.keras.applications.InceptionV3(include_top=False,\n",
    "                   input_shape=(row,col,3),\n",
    "                   pooling='avg',classes=4,\n",
    "                   weights='imagenet')\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "model1.add(pretrained_model)\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Flatten())\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(2048,kernel_initializer='he_uniform'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(1024,kernel_initializer='he_uniform'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(4,activation='softmax'))\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = tf.keras.metrics.AUC(name = 'acc')\n",
    "from tensorflow.keras.optimizers import Adam # optimizer hyperparameter\n",
    "model1.compile(optimizer=Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=METRIC) #learning rate hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_resnet_v2 (Functio (None, 1536)              54336736  \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_906 (Bat (None, 1536)              6144      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2048)              3147776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_907 (Bat (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "activation_901 (Activation)  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_908 (Bat (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_902 (Activation)  (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 59,605,220\n",
      "Trainable params: 5,259,268\n",
      "Non-trainable params: 54,345,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, MaxPooling2D, Activation, BatchNormalization\n",
    "import tensorflow as tf\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "pretrained_model= tf.keras.applications.InceptionResNetV2(include_top=False,\n",
    "                   input_shape=(row,col,3),\n",
    "                   pooling='avg',classes=4,\n",
    "                   weights='imagenet')\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "model2.add(pretrained_model)\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Flatten())\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(2048,kernel_initializer='he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1024,kernel_initializer='he_uniform'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(4,activation='softmax'))\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = tf.keras.metrics.AUC(name = 'acc')\n",
    "from tensorflow.keras.optimizers import Adam # optimizer hyperparameter\n",
    "model2.compile(optimizer=Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=METRIC) #learning rate hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'best_weights.hdf5'\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "earlystopping = EarlyStopping(monitor = 'acc', \n",
    "                              mode = 'max' , \n",
    "                              patience = 15,\n",
    "                              verbose = 1)\n",
    "\n",
    "checkpoint    = ModelCheckpoint(filepath, \n",
    "                                monitor = 'acc', \n",
    "                                mode='max', \n",
    "                                save_best_only=True, \n",
    "                                verbose = 1)\n",
    "callback_list = [earlystopping, checkpoint]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the InceptionV3 model\n",
      "Epoch 1/200\n",
      "64/64 [==============================] - 13s 162ms/step - loss: 1.3013 - acc: 0.7753 - val_loss: 2.7030 - val_acc: 0.6756\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.77534, saving model to best_weights.hdf5\n",
      "Epoch 2/200\n",
      "64/64 [==============================] - 9s 145ms/step - loss: 0.9432 - acc: 0.8590 - val_loss: 1.1726 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00002: acc improved from 0.77534 to 0.85896, saving model to best_weights.hdf5\n",
      "Epoch 3/200\n",
      "64/64 [==============================] - 9s 145ms/step - loss: 0.8203 - acc: 0.8849 - val_loss: 0.7276 - val_acc: 0.9076\n",
      "\n",
      "Epoch 00003: acc improved from 0.85896 to 0.88486, saving model to best_weights.hdf5\n",
      "Epoch 4/200\n",
      "64/64 [==============================] - 9s 145ms/step - loss: 0.7566 - acc: 0.8997 - val_loss: 0.6305 - val_acc: 0.9292\n",
      "\n",
      "Epoch 00004: acc improved from 0.88486 to 0.89966, saving model to best_weights.hdf5\n",
      "Epoch 5/200\n",
      "64/64 [==============================] - 9s 146ms/step - loss: 0.7204 - acc: 0.9062 - val_loss: 0.6044 - val_acc: 0.9353\n",
      "\n",
      "Epoch 00005: acc improved from 0.89966 to 0.90622, saving model to best_weights.hdf5\n",
      "Epoch 6/200\n",
      "64/64 [==============================] - 9s 146ms/step - loss: 0.6909 - acc: 0.9142 - val_loss: 0.5670 - val_acc: 0.9415\n",
      "\n",
      "Epoch 00006: acc improved from 0.90622 to 0.91420, saving model to best_weights.hdf5\n",
      "Epoch 7/200\n",
      "64/64 [==============================] - 9s 148ms/step - loss: 0.6608 - acc: 0.9206 - val_loss: 0.5479 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00007: acc improved from 0.91420 to 0.92064, saving model to best_weights.hdf5\n",
      "Epoch 8/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.6380 - acc: 0.9264 - val_loss: 0.5281 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00008: acc improved from 0.92064 to 0.92642, saving model to best_weights.hdf5\n",
      "Epoch 9/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.6224 - acc: 0.9293 - val_loss: 0.5298 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00009: acc improved from 0.92642 to 0.92931, saving model to best_weights.hdf5\n",
      "Epoch 10/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.6092 - acc: 0.9328 - val_loss: 0.5128 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00010: acc improved from 0.92931 to 0.93278, saving model to best_weights.hdf5\n",
      "Epoch 11/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.5986 - acc: 0.9354 - val_loss: 0.4947 - val_acc: 0.9562\n",
      "\n",
      "Epoch 00011: acc improved from 0.93278 to 0.93538, saving model to best_weights.hdf5\n",
      "Epoch 12/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.5914 - acc: 0.9364 - val_loss: 0.4885 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00012: acc improved from 0.93538 to 0.93643, saving model to best_weights.hdf5\n",
      "Epoch 13/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.5692 - acc: 0.9413 - val_loss: 0.4725 - val_acc: 0.9597\n",
      "\n",
      "Epoch 00013: acc improved from 0.93643 to 0.94127, saving model to best_weights.hdf5\n",
      "Epoch 14/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.5515 - acc: 0.9450 - val_loss: 0.4487 - val_acc: 0.9642\n",
      "\n",
      "Epoch 00014: acc improved from 0.94127 to 0.94496, saving model to best_weights.hdf5\n",
      "Epoch 15/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.5384 - acc: 0.9474 - val_loss: 0.4592 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00015: acc improved from 0.94496 to 0.94737, saving model to best_weights.hdf5\n",
      "Epoch 16/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.5440 - acc: 0.9463 - val_loss: 0.4511 - val_acc: 0.9633\n",
      "\n",
      "Epoch 00016: acc did not improve from 0.94737\n",
      "Epoch 17/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.5171 - acc: 0.9513 - val_loss: 0.4425 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00017: acc improved from 0.94737 to 0.95127, saving model to best_weights.hdf5\n",
      "Epoch 18/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.5194 - acc: 0.9512 - val_loss: 0.4389 - val_acc: 0.9651\n",
      "\n",
      "Epoch 00018: acc did not improve from 0.95127\n",
      "Epoch 19/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.4973 - acc: 0.9549 - val_loss: 0.4132 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00019: acc improved from 0.95127 to 0.95488, saving model to best_weights.hdf5\n",
      "Epoch 20/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4999 - acc: 0.9547 - val_loss: 0.4288 - val_acc: 0.9665\n",
      "\n",
      "Epoch 00020: acc did not improve from 0.95488\n",
      "Epoch 21/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.4856 - acc: 0.9572 - val_loss: 0.4119 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00021: acc improved from 0.95488 to 0.95724, saving model to best_weights.hdf5\n",
      "Epoch 22/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4714 - acc: 0.9596 - val_loss: 0.3933 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00022: acc improved from 0.95724 to 0.95963, saving model to best_weights.hdf5\n",
      "Epoch 23/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4684 - acc: 0.9602 - val_loss: 0.3914 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00023: acc improved from 0.95963 to 0.96017, saving model to best_weights.hdf5\n",
      "Epoch 24/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.4566 - acc: 0.9622 - val_loss: 0.3980 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00024: acc improved from 0.96017 to 0.96220, saving model to best_weights.hdf5\n",
      "Epoch 25/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4491 - acc: 0.9634 - val_loss: 0.3856 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00025: acc improved from 0.96220 to 0.96342, saving model to best_weights.hdf5\n",
      "Epoch 26/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4459 - acc: 0.9640 - val_loss: 0.3995 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00026: acc improved from 0.96342 to 0.96399, saving model to best_weights.hdf5\n",
      "Epoch 27/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4433 - acc: 0.9644 - val_loss: 0.3729 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00027: acc improved from 0.96399 to 0.96438, saving model to best_weights.hdf5\n",
      "Epoch 28/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4187 - acc: 0.9680 - val_loss: 0.3727 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00028: acc improved from 0.96438 to 0.96799, saving model to best_weights.hdf5\n",
      "Epoch 29/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4215 - acc: 0.9677 - val_loss: 0.3643 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00029: acc did not improve from 0.96799\n",
      "Epoch 30/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.4066 - acc: 0.9700 - val_loss: 0.3631 - val_acc: 0.9758\n",
      "\n",
      "Epoch 00030: acc improved from 0.96799 to 0.96999, saving model to best_weights.hdf5\n",
      "Epoch 31/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.4107 - acc: 0.9693 - val_loss: 0.3452 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00031: acc did not improve from 0.96999\n",
      "Epoch 32/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.3993 - acc: 0.9710 - val_loss: 0.3456 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00032: acc improved from 0.96999 to 0.97096, saving model to best_weights.hdf5\n",
      "Epoch 33/200\n",
      "64/64 [==============================] - 10s 153ms/step - loss: 0.3887 - acc: 0.9725 - val_loss: 0.3524 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00033: acc improved from 0.97096 to 0.97246, saving model to best_weights.hdf5\n",
      "Epoch 34/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.3713 - acc: 0.9748 - val_loss: 0.3394 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00034: acc improved from 0.97246 to 0.97484, saving model to best_weights.hdf5\n",
      "Epoch 35/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.3754 - acc: 0.9743 - val_loss: 0.3351 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00035: acc did not improve from 0.97484\n",
      "Epoch 36/200\n",
      "64/64 [==============================] - 10s 156ms/step - loss: 0.3692 - acc: 0.9751 - val_loss: 0.3296 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00036: acc improved from 0.97484 to 0.97509, saving model to best_weights.hdf5\n",
      "Epoch 37/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.3566 - acc: 0.9765 - val_loss: 0.3245 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00037: acc improved from 0.97509 to 0.97651, saving model to best_weights.hdf5\n",
      "Epoch 38/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.3444 - acc: 0.9782 - val_loss: 0.3284 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00038: acc improved from 0.97651 to 0.97817, saving model to best_weights.hdf5\n",
      "Epoch 39/200\n",
      "64/64 [==============================] - 10s 156ms/step - loss: 0.3494 - acc: 0.9774 - val_loss: 0.3245 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00039: acc did not improve from 0.97817\n",
      "Epoch 40/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.3424 - acc: 0.9785 - val_loss: 0.3142 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00040: acc improved from 0.97817 to 0.97846, saving model to best_weights.hdf5\n",
      "Epoch 41/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.3345 - acc: 0.9794 - val_loss: 0.3119 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00041: acc improved from 0.97846 to 0.97941, saving model to best_weights.hdf5\n",
      "Epoch 42/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.3266 - acc: 0.9802 - val_loss: 0.3247 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00042: acc improved from 0.97941 to 0.98019, saving model to best_weights.hdf5\n",
      "Epoch 43/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.3296 - acc: 0.9800 - val_loss: 0.3321 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00043: acc did not improve from 0.98019\n",
      "Epoch 44/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.3168 - acc: 0.9815 - val_loss: 0.3096 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00044: acc improved from 0.98019 to 0.98150, saving model to best_weights.hdf5\n",
      "Epoch 45/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.3099 - acc: 0.9822 - val_loss: 0.3089 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00045: acc improved from 0.98150 to 0.98219, saving model to best_weights.hdf5\n",
      "Epoch 46/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.3121 - acc: 0.9818 - val_loss: 0.2947 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00046: acc did not improve from 0.98219\n",
      "Epoch 47/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.3130 - acc: 0.9818 - val_loss: 0.2913 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00047: acc did not improve from 0.98219\n",
      "Epoch 48/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.3042 - acc: 0.9828 - val_loss: 0.2980 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00048: acc improved from 0.98219 to 0.98283, saving model to best_weights.hdf5\n",
      "Epoch 49/200\n",
      "64/64 [==============================] - 10s 153ms/step - loss: 0.2842 - acc: 0.9849 - val_loss: 0.2878 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00049: acc improved from 0.98283 to 0.98489, saving model to best_weights.hdf5\n",
      "Epoch 50/200\n",
      "64/64 [==============================] - 10s 160ms/step - loss: 0.2818 - acc: 0.9850 - val_loss: 0.2918 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00050: acc improved from 0.98489 to 0.98500, saving model to best_weights.hdf5\n",
      "Epoch 51/200\n",
      "64/64 [==============================] - 9s 148ms/step - loss: 0.2950 - acc: 0.9838 - val_loss: 0.2801 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00051: acc did not improve from 0.98500\n",
      "Epoch 52/200\n",
      "64/64 [==============================] - 10s 158ms/step - loss: 0.2891 - acc: 0.9844 - val_loss: 0.2820 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00052: acc did not improve from 0.98500\n",
      "Epoch 53/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.2704 - acc: 0.9864 - val_loss: 0.2943 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00053: acc improved from 0.98500 to 0.98638, saving model to best_weights.hdf5\n",
      "Epoch 54/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.2668 - acc: 0.9865 - val_loss: 0.2888 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00054: acc improved from 0.98638 to 0.98650, saving model to best_weights.hdf5\n",
      "Epoch 55/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.2815 - acc: 0.9853 - val_loss: 0.2890 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00055: acc did not improve from 0.98650\n",
      "Epoch 56/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.2585 - acc: 0.9873 - val_loss: 0.2751 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00056: acc improved from 0.98650 to 0.98726, saving model to best_weights.hdf5\n",
      "Epoch 57/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.2641 - acc: 0.9868 - val_loss: 0.2862 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00057: acc did not improve from 0.98726\n",
      "Epoch 58/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.2592 - acc: 0.9874 - val_loss: 0.2719 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00058: acc improved from 0.98726 to 0.98741, saving model to best_weights.hdf5\n",
      "Epoch 59/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.2531 - acc: 0.9877 - val_loss: 0.2754 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00059: acc improved from 0.98741 to 0.98771, saving model to best_weights.hdf5\n",
      "Epoch 60/200\n",
      "64/64 [==============================] - 10s 153ms/step - loss: 0.2531 - acc: 0.9879 - val_loss: 0.2836 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00060: acc improved from 0.98771 to 0.98787, saving model to best_weights.hdf5\n",
      "Epoch 61/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.2425 - acc: 0.9887 - val_loss: 0.2773 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00061: acc improved from 0.98787 to 0.98872, saving model to best_weights.hdf5\n",
      "Epoch 62/200\n",
      "64/64 [==============================] - 9s 149ms/step - loss: 0.2474 - acc: 0.9882 - val_loss: 0.2720 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00062: acc did not improve from 0.98872\n",
      "Epoch 63/200\n",
      "64/64 [==============================] - 9s 149ms/step - loss: 0.2434 - acc: 0.9885 - val_loss: 0.2835 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00063: acc did not improve from 0.98872\n",
      "Epoch 64/200\n",
      "64/64 [==============================] - 9s 148ms/step - loss: 0.2298 - acc: 0.9899 - val_loss: 0.2738 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00064: acc improved from 0.98872 to 0.98988, saving model to best_weights.hdf5\n",
      "Epoch 65/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.2391 - acc: 0.9890 - val_loss: 0.2672 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00065: acc did not improve from 0.98988\n",
      "Epoch 66/200\n",
      "64/64 [==============================] - 9s 146ms/step - loss: 0.2317 - acc: 0.9894 - val_loss: 0.2645 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00066: acc did not improve from 0.98988\n",
      "Epoch 67/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.2270 - acc: 0.9901 - val_loss: 0.2721 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00067: acc improved from 0.98988 to 0.99010, saving model to best_weights.hdf5\n",
      "Epoch 68/200\n",
      "64/64 [==============================] - 9s 148ms/step - loss: 0.2347 - acc: 0.9893 - val_loss: 0.2627 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00068: acc did not improve from 0.99010\n",
      "Epoch 69/200\n",
      "64/64 [==============================] - 9s 148ms/step - loss: 0.2273 - acc: 0.9900 - val_loss: 0.2652 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00069: acc did not improve from 0.99010\n",
      "Epoch 70/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.2201 - acc: 0.9906 - val_loss: 0.2659 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00070: acc improved from 0.99010 to 0.99056, saving model to best_weights.hdf5\n",
      "Epoch 71/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.2153 - acc: 0.9909 - val_loss: 0.2592 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00071: acc improved from 0.99056 to 0.99092, saving model to best_weights.hdf5\n",
      "Epoch 72/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.2161 - acc: 0.9906 - val_loss: 0.2543 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00072: acc did not improve from 0.99092\n",
      "Epoch 73/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.2120 - acc: 0.9911 - val_loss: 0.2626 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00073: acc improved from 0.99092 to 0.99109, saving model to best_weights.hdf5\n",
      "Epoch 74/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.2182 - acc: 0.9907 - val_loss: 0.2702 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00074: acc did not improve from 0.99109\n",
      "Epoch 75/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.2167 - acc: 0.9909 - val_loss: 0.2601 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00075: acc did not improve from 0.99109\n",
      "Epoch 76/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.2066 - acc: 0.9915 - val_loss: 0.2652 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00076: acc improved from 0.99109 to 0.99147, saving model to best_weights.hdf5\n",
      "Epoch 77/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.1991 - acc: 0.9922 - val_loss: 0.2514 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00077: acc improved from 0.99147 to 0.99218, saving model to best_weights.hdf5\n",
      "Epoch 78/200\n",
      "64/64 [==============================] - 9s 146ms/step - loss: 0.2031 - acc: 0.9916 - val_loss: 0.2598 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00078: acc did not improve from 0.99218\n",
      "Epoch 79/200\n",
      "64/64 [==============================] - 9s 147ms/step - loss: 0.1984 - acc: 0.9922 - val_loss: 0.2521 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00079: acc improved from 0.99218 to 0.99225, saving model to best_weights.hdf5\n",
      "Epoch 80/200\n",
      "64/64 [==============================] - 9s 146ms/step - loss: 0.1881 - acc: 0.9928 - val_loss: 0.2439 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00080: acc improved from 0.99225 to 0.99277, saving model to best_weights.hdf5\n",
      "Epoch 81/200\n",
      "64/64 [==============================] - 9s 148ms/step - loss: 0.2027 - acc: 0.9917 - val_loss: 0.2612 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00081: acc did not improve from 0.99277\n",
      "Epoch 82/200\n",
      "64/64 [==============================] - 10s 149ms/step - loss: 0.1965 - acc: 0.9924 - val_loss: 0.2558 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00082: acc did not improve from 0.99277\n",
      "Epoch 83/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.1952 - acc: 0.9924 - val_loss: 0.2597 - val_acc: 0.9876\n",
      "\n",
      "Epoch 00083: acc did not improve from 0.99277\n",
      "Epoch 84/200\n",
      "64/64 [==============================] - 9s 149ms/step - loss: 0.1954 - acc: 0.9923 - val_loss: 0.2542 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00084: acc did not improve from 0.99277\n",
      "Epoch 85/200\n",
      "64/64 [==============================] - 9s 149ms/step - loss: 0.1876 - acc: 0.9929 - val_loss: 0.2595 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00085: acc improved from 0.99277 to 0.99294, saving model to best_weights.hdf5\n",
      "Epoch 86/200\n",
      "64/64 [==============================] - 9s 149ms/step - loss: 0.1812 - acc: 0.9931 - val_loss: 0.2740 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00086: acc improved from 0.99294 to 0.99308, saving model to best_weights.hdf5\n",
      "Epoch 87/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1918 - acc: 0.9926 - val_loss: 0.2480 - val_acc: 0.9881\n",
      "\n",
      "Epoch 00087: acc did not improve from 0.99308\n",
      "Epoch 88/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1783 - acc: 0.9937 - val_loss: 0.2471 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00088: acc improved from 0.99308 to 0.99369, saving model to best_weights.hdf5\n",
      "Epoch 89/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1855 - acc: 0.9929 - val_loss: 0.2430 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00089: acc did not improve from 0.99369\n",
      "Epoch 90/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1756 - acc: 0.9939 - val_loss: 0.2447 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00090: acc improved from 0.99369 to 0.99388, saving model to best_weights.hdf5\n",
      "Epoch 91/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1701 - acc: 0.9942 - val_loss: 0.2613 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00091: acc improved from 0.99388 to 0.99418, saving model to best_weights.hdf5\n",
      "Epoch 92/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1800 - acc: 0.9937 - val_loss: 0.2485 - val_acc: 0.9881\n",
      "\n",
      "Epoch 00092: acc did not improve from 0.99418\n",
      "Epoch 93/200\n",
      "64/64 [==============================] - 10s 153ms/step - loss: 0.1690 - acc: 0.9941 - val_loss: 0.2591 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00093: acc did not improve from 0.99418\n",
      "Epoch 94/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1759 - acc: 0.9936 - val_loss: 0.2403 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00094: acc did not improve from 0.99418\n",
      "Epoch 95/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1633 - acc: 0.9946 - val_loss: 0.2492 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00095: acc improved from 0.99418 to 0.99464, saving model to best_weights.hdf5\n",
      "Epoch 96/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1611 - acc: 0.9944 - val_loss: 0.2540 - val_acc: 0.9881\n",
      "\n",
      "Epoch 00096: acc did not improve from 0.99464\n",
      "Epoch 97/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1645 - acc: 0.9945 - val_loss: 0.2588 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00097: acc did not improve from 0.99464\n",
      "Epoch 98/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1668 - acc: 0.9944 - val_loss: 0.2577 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00098: acc did not improve from 0.99464\n",
      "Epoch 99/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1627 - acc: 0.9943 - val_loss: 0.2474 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00099: acc did not improve from 0.99464\n",
      "Epoch 100/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1657 - acc: 0.9945 - val_loss: 0.2489 - val_acc: 0.9879\n",
      "\n",
      "Epoch 00100: acc did not improve from 0.99464\n",
      "Epoch 101/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1539 - acc: 0.9952 - val_loss: 0.2444 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00101: acc improved from 0.99464 to 0.99518, saving model to best_weights.hdf5\n",
      "Epoch 102/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1433 - acc: 0.9958 - val_loss: 0.2436 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00102: acc improved from 0.99518 to 0.99579, saving model to best_weights.hdf5\n",
      "Epoch 103/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1504 - acc: 0.9951 - val_loss: 0.2502 - val_acc: 0.9878\n",
      "\n",
      "Epoch 00103: acc did not improve from 0.99579\n",
      "Epoch 104/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.1640 - acc: 0.9947 - val_loss: 0.2485 - val_acc: 0.9879\n",
      "\n",
      "Epoch 00104: acc did not improve from 0.99579\n",
      "Epoch 105/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1429 - acc: 0.9957 - val_loss: 0.2447 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00105: acc did not improve from 0.99579\n",
      "Epoch 106/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1503 - acc: 0.9951 - val_loss: 0.2523 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00106: acc did not improve from 0.99579\n",
      "Epoch 107/200\n",
      "64/64 [==============================] - 10s 153ms/step - loss: 0.1421 - acc: 0.9956 - val_loss: 0.2552 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00107: acc did not improve from 0.99579\n",
      "Epoch 108/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1417 - acc: 0.9955 - val_loss: 0.2465 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00108: acc did not improve from 0.99579\n",
      "Epoch 109/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1438 - acc: 0.9955 - val_loss: 0.2331 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00109: acc did not improve from 0.99579\n",
      "Epoch 110/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1414 - acc: 0.9959 - val_loss: 0.2427 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00110: acc improved from 0.99579 to 0.99590, saving model to best_weights.hdf5\n",
      "Epoch 111/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1526 - acc: 0.9952 - val_loss: 0.2523 - val_acc: 0.9879\n",
      "\n",
      "Epoch 00111: acc did not improve from 0.99590\n",
      "Epoch 112/200\n",
      "64/64 [==============================] - 10s 150ms/step - loss: 0.1408 - acc: 0.9960 - val_loss: 0.2378 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00112: acc improved from 0.99590 to 0.99597, saving model to best_weights.hdf5\n",
      "Epoch 113/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1436 - acc: 0.9955 - val_loss: 0.2409 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00113: acc did not improve from 0.99597\n",
      "Epoch 114/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1287 - acc: 0.9963 - val_loss: 0.2444 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00114: acc improved from 0.99597 to 0.99631, saving model to best_weights.hdf5\n",
      "Epoch 115/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1350 - acc: 0.9961 - val_loss: 0.2375 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00115: acc did not improve from 0.99631\n",
      "Epoch 116/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1490 - acc: 0.9951 - val_loss: 0.2471 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00116: acc did not improve from 0.99631\n",
      "Epoch 117/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1466 - acc: 0.9953 - val_loss: 0.2369 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00117: acc did not improve from 0.99631\n",
      "Epoch 118/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1399 - acc: 0.9958 - val_loss: 0.2416 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00118: acc did not improve from 0.99631\n",
      "Epoch 119/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1413 - acc: 0.9958 - val_loss: 0.2460 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00119: acc did not improve from 0.99631\n",
      "Epoch 120/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1228 - acc: 0.9969 - val_loss: 0.2372 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00120: acc improved from 0.99631 to 0.99691, saving model to best_weights.hdf5\n",
      "Epoch 121/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1332 - acc: 0.9959 - val_loss: 0.2367 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00121: acc did not improve from 0.99691\n",
      "Epoch 122/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1307 - acc: 0.9964 - val_loss: 0.2374 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00122: acc did not improve from 0.99691\n",
      "Epoch 123/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1258 - acc: 0.9964 - val_loss: 0.2374 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00123: acc did not improve from 0.99691\n",
      "Epoch 124/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1322 - acc: 0.9962 - val_loss: 0.2324 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00124: acc did not improve from 0.99691\n",
      "Epoch 125/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1210 - acc: 0.9966 - val_loss: 0.2537 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00125: acc did not improve from 0.99691\n",
      "Epoch 126/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1165 - acc: 0.9969 - val_loss: 0.2384 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00126: acc improved from 0.99691 to 0.99695, saving model to best_weights.hdf5\n",
      "Epoch 127/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1411 - acc: 0.9956 - val_loss: 0.2366 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00127: acc did not improve from 0.99695\n",
      "Epoch 128/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1292 - acc: 0.9960 - val_loss: 0.2534 - val_acc: 0.9876\n",
      "\n",
      "Epoch 00128: acc did not improve from 0.99695\n",
      "Epoch 129/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1176 - acc: 0.9969 - val_loss: 0.2475 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00129: acc did not improve from 0.99695\n",
      "Epoch 130/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1267 - acc: 0.9963 - val_loss: 0.2440 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00130: acc did not improve from 0.99695\n",
      "Epoch 131/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1288 - acc: 0.9959 - val_loss: 0.2369 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00131: acc did not improve from 0.99695\n",
      "Epoch 132/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1136 - acc: 0.9969 - val_loss: 0.2346 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00132: acc did not improve from 0.99695\n",
      "Epoch 133/200\n",
      "64/64 [==============================] - 10s 153ms/step - loss: 0.1230 - acc: 0.9966 - val_loss: 0.2326 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00133: acc did not improve from 0.99695\n",
      "Epoch 134/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1187 - acc: 0.9965 - val_loss: 0.2349 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00134: acc did not improve from 0.99695\n",
      "Epoch 135/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1184 - acc: 0.9969 - val_loss: 0.2233 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00135: acc did not improve from 0.99695\n",
      "Epoch 136/200\n",
      "64/64 [==============================] - 10s 157ms/step - loss: 0.1109 - acc: 0.9973 - val_loss: 0.2301 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00136: acc improved from 0.99695 to 0.99727, saving model to best_weights.hdf5\n",
      "Epoch 137/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.1145 - acc: 0.9969 - val_loss: 0.2502 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00137: acc did not improve from 0.99727\n",
      "Epoch 138/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1235 - acc: 0.9965 - val_loss: 0.2374 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00138: acc did not improve from 0.99727\n",
      "Epoch 139/200\n",
      "64/64 [==============================] - 10s 153ms/step - loss: 0.1177 - acc: 0.9968 - val_loss: 0.2378 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00139: acc did not improve from 0.99727\n",
      "Epoch 140/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1106 - acc: 0.9974 - val_loss: 0.2262 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00140: acc improved from 0.99727 to 0.99742, saving model to best_weights.hdf5\n",
      "Epoch 141/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1203 - acc: 0.9965 - val_loss: 0.2313 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00141: acc did not improve from 0.99742\n",
      "Epoch 142/200\n",
      "64/64 [==============================] - 10s 154ms/step - loss: 0.1102 - acc: 0.9971 - val_loss: 0.2378 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00142: acc did not improve from 0.99742\n",
      "Epoch 143/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1193 - acc: 0.9968 - val_loss: 0.2376 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00143: acc did not improve from 0.99742\n",
      "Epoch 144/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1199 - acc: 0.9964 - val_loss: 0.2463 - val_acc: 0.9881\n",
      "\n",
      "Epoch 00144: acc did not improve from 0.99742\n",
      "Epoch 145/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1077 - acc: 0.9975 - val_loss: 0.2385 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00145: acc improved from 0.99742 to 0.99748, saving model to best_weights.hdf5\n",
      "Epoch 146/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1005 - acc: 0.9978 - val_loss: 0.2249 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00146: acc improved from 0.99748 to 0.99775, saving model to best_weights.hdf5\n",
      "Epoch 147/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1101 - acc: 0.9973 - val_loss: 0.2276 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00147: acc did not improve from 0.99775\n",
      "Epoch 148/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.0962 - acc: 0.9979 - val_loss: 0.2368 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00148: acc improved from 0.99775 to 0.99794, saving model to best_weights.hdf5\n",
      "Epoch 149/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1053 - acc: 0.9975 - val_loss: 0.2393 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00149: acc did not improve from 0.99794\n",
      "Epoch 150/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1175 - acc: 0.9967 - val_loss: 0.2423 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00150: acc did not improve from 0.99794\n",
      "Epoch 151/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.1107 - acc: 0.9973 - val_loss: 0.2449 - val_acc: 0.9887\n",
      "\n",
      "Epoch 00151: acc did not improve from 0.99794\n",
      "Epoch 152/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1130 - acc: 0.9971 - val_loss: 0.2360 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00152: acc did not improve from 0.99794\n",
      "Epoch 153/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1100 - acc: 0.9969 - val_loss: 0.2335 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00153: acc did not improve from 0.99794\n",
      "Epoch 154/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1033 - acc: 0.9975 - val_loss: 0.2401 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00154: acc did not improve from 0.99794\n",
      "Epoch 155/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1008 - acc: 0.9977 - val_loss: 0.2312 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00155: acc did not improve from 0.99794\n",
      "Epoch 156/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1099 - acc: 0.9971 - val_loss: 0.2322 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00156: acc did not improve from 0.99794\n",
      "Epoch 157/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1079 - acc: 0.9969 - val_loss: 0.2339 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00157: acc did not improve from 0.99794\n",
      "Epoch 158/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1063 - acc: 0.9976 - val_loss: 0.2380 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00158: acc did not improve from 0.99794\n",
      "Epoch 159/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1108 - acc: 0.9973 - val_loss: 0.2324 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00159: acc did not improve from 0.99794\n",
      "Epoch 160/200\n",
      "64/64 [==============================] - 10s 151ms/step - loss: 0.1014 - acc: 0.9973 - val_loss: 0.2354 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00160: acc did not improve from 0.99794\n",
      "Epoch 161/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.0976 - acc: 0.9977 - val_loss: 0.2346 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00161: acc did not improve from 0.99794\n",
      "Epoch 162/200\n",
      "64/64 [==============================] - 10s 155ms/step - loss: 0.0957 - acc: 0.9977 - val_loss: 0.2278 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00162: acc did not improve from 0.99794\n",
      "Epoch 163/200\n",
      "64/64 [==============================] - 10s 152ms/step - loss: 0.0981 - acc: 0.9975 - val_loss: 0.2446 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00163: acc did not improve from 0.99794\n",
      "Epoch 00163: early stopping\n",
      "Training the InceptionResNet model\n",
      "Epoch 1/200\n",
      "64/64 [==============================] - 33s 415ms/step - loss: 1.3945 - acc: 0.7411 - val_loss: 0.8761 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00001: acc did not improve from 0.99794\n",
      "Epoch 2/200\n",
      "64/64 [==============================] - 24s 383ms/step - loss: 1.0585 - acc: 0.8263 - val_loss: 0.8257 - val_acc: 0.8825\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.99794\n",
      "Epoch 3/200\n",
      "64/64 [==============================] - 24s 380ms/step - loss: 0.9317 - acc: 0.8513 - val_loss: 0.7601 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00003: acc did not improve from 0.99794\n",
      "Epoch 4/200\n",
      "64/64 [==============================] - 24s 379ms/step - loss: 0.8653 - acc: 0.8672 - val_loss: 0.7426 - val_acc: 0.9014\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.99794\n",
      "Epoch 5/200\n",
      "64/64 [==============================] - 24s 377ms/step - loss: 0.8386 - acc: 0.8744 - val_loss: 0.6955 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.99794\n",
      "Epoch 6/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.8003 - acc: 0.8838 - val_loss: 0.6772 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.99794\n",
      "Epoch 7/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.7774 - acc: 0.8900 - val_loss: 0.6770 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.99794\n",
      "Epoch 8/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.7705 - acc: 0.8918 - val_loss: 0.6665 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.99794\n",
      "Epoch 9/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.7587 - acc: 0.8947 - val_loss: 0.6454 - val_acc: 0.9220\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.99794\n",
      "Epoch 10/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.7406 - acc: 0.8999 - val_loss: 0.6347 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.99794\n",
      "Epoch 11/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.7160 - acc: 0.9061 - val_loss: 0.6156 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00011: acc did not improve from 0.99794\n",
      "Epoch 12/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.7276 - acc: 0.9039 - val_loss: 0.6119 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00012: acc did not improve from 0.99794\n",
      "Epoch 13/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.7033 - acc: 0.9093 - val_loss: 0.6056 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00013: acc did not improve from 0.99794\n",
      "Epoch 14/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.6921 - acc: 0.9124 - val_loss: 0.6137 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00014: acc did not improve from 0.99794\n",
      "Epoch 15/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.6807 - acc: 0.9155 - val_loss: 0.5909 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00015: acc did not improve from 0.99794\n",
      "Epoch 16/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.6851 - acc: 0.9143 - val_loss: 0.5837 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00016: acc did not improve from 0.99794\n",
      "Epoch 17/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.6758 - acc: 0.9167 - val_loss: 0.5663 - val_acc: 0.9415\n",
      "\n",
      "Epoch 00017: acc did not improve from 0.99794\n",
      "Epoch 18/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6765 - acc: 0.9167 - val_loss: 0.6037 - val_acc: 0.9332\n",
      "\n",
      "Epoch 00018: acc did not improve from 0.99794\n",
      "Epoch 19/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.6533 - acc: 0.9223 - val_loss: 0.5606 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00019: acc did not improve from 0.99794\n",
      "Epoch 20/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6504 - acc: 0.9227 - val_loss: 0.5553 - val_acc: 0.9435\n",
      "\n",
      "Epoch 00020: acc did not improve from 0.99794\n",
      "Epoch 21/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6425 - acc: 0.9246 - val_loss: 0.5478 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00021: acc did not improve from 0.99794\n",
      "Epoch 22/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6408 - acc: 0.9251 - val_loss: 0.5483 - val_acc: 0.9448\n",
      "\n",
      "Epoch 00022: acc did not improve from 0.99794\n",
      "Epoch 23/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6383 - acc: 0.9260 - val_loss: 0.5398 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00023: acc did not improve from 0.99794\n",
      "Epoch 24/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6220 - acc: 0.9291 - val_loss: 0.5322 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00024: acc did not improve from 0.99794\n",
      "Epoch 25/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.6193 - acc: 0.9306 - val_loss: 0.5253 - val_acc: 0.9502\n",
      "\n",
      "Epoch 00025: acc did not improve from 0.99794\n",
      "Epoch 26/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6187 - acc: 0.9298 - val_loss: 0.5448 - val_acc: 0.9460\n",
      "\n",
      "Epoch 00026: acc did not improve from 0.99794\n",
      "Epoch 27/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6040 - acc: 0.9336 - val_loss: 0.5155 - val_acc: 0.9527\n",
      "\n",
      "Epoch 00027: acc did not improve from 0.99794\n",
      "Epoch 28/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6040 - acc: 0.9337 - val_loss: 0.5222 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00028: acc did not improve from 0.99794\n",
      "Epoch 29/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.6004 - acc: 0.9343 - val_loss: 0.5123 - val_acc: 0.9527\n",
      "\n",
      "Epoch 00029: acc did not improve from 0.99794\n",
      "Epoch 30/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5823 - acc: 0.9383 - val_loss: 0.4998 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00030: acc did not improve from 0.99794\n",
      "Epoch 31/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5842 - acc: 0.9378 - val_loss: 0.5004 - val_acc: 0.9553\n",
      "\n",
      "Epoch 00031: acc did not improve from 0.99794\n",
      "Epoch 32/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5765 - acc: 0.9396 - val_loss: 0.4963 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00032: acc did not improve from 0.99794\n",
      "Epoch 33/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5643 - acc: 0.9426 - val_loss: 0.5196 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00033: acc did not improve from 0.99794\n",
      "Epoch 34/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5734 - acc: 0.9405 - val_loss: 0.4822 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00034: acc did not improve from 0.99794\n",
      "Epoch 35/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5660 - acc: 0.9419 - val_loss: 0.4795 - val_acc: 0.9588\n",
      "\n",
      "Epoch 00035: acc did not improve from 0.99794\n",
      "Epoch 36/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5703 - acc: 0.9409 - val_loss: 0.4847 - val_acc: 0.9575\n",
      "\n",
      "Epoch 00036: acc did not improve from 0.99794\n",
      "Epoch 37/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5531 - acc: 0.9447 - val_loss: 0.4741 - val_acc: 0.9598\n",
      "\n",
      "Epoch 00037: acc did not improve from 0.99794\n",
      "Epoch 38/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5515 - acc: 0.9446 - val_loss: 0.4834 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00038: acc did not improve from 0.99794\n",
      "Epoch 39/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5469 - acc: 0.9460 - val_loss: 0.4628 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00039: acc did not improve from 0.99794\n",
      "Epoch 40/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5491 - acc: 0.9454 - val_loss: 0.4626 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00040: acc did not improve from 0.99794\n",
      "Epoch 41/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5464 - acc: 0.9462 - val_loss: 0.4546 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00041: acc did not improve from 0.99794\n",
      "Epoch 42/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5249 - acc: 0.9503 - val_loss: 0.4677 - val_acc: 0.9605\n",
      "\n",
      "Epoch 00042: acc did not improve from 0.99794\n",
      "Epoch 43/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.5294 - acc: 0.9491 - val_loss: 0.4458 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00043: acc did not improve from 0.99794\n",
      "Epoch 44/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5270 - acc: 0.9499 - val_loss: 0.4561 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00044: acc did not improve from 0.99794\n",
      "Epoch 45/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5318 - acc: 0.9487 - val_loss: 0.4441 - val_acc: 0.9648\n",
      "\n",
      "Epoch 00045: acc did not improve from 0.99794\n",
      "Epoch 46/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5209 - acc: 0.9507 - val_loss: 0.4468 - val_acc: 0.9642\n",
      "\n",
      "Epoch 00046: acc did not improve from 0.99794\n",
      "Epoch 47/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.5080 - acc: 0.9533 - val_loss: 0.4355 - val_acc: 0.9657\n",
      "\n",
      "Epoch 00047: acc did not improve from 0.99794\n",
      "Epoch 48/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5087 - acc: 0.9532 - val_loss: 0.4323 - val_acc: 0.9665\n",
      "\n",
      "Epoch 00048: acc did not improve from 0.99794\n",
      "Epoch 49/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5017 - acc: 0.9542 - val_loss: 0.4376 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00049: acc did not improve from 0.99794\n",
      "Epoch 50/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.5153 - acc: 0.9520 - val_loss: 0.4237 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00050: acc did not improve from 0.99794\n",
      "Epoch 51/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.5077 - acc: 0.9534 - val_loss: 0.4213 - val_acc: 0.9685\n",
      "\n",
      "Epoch 00051: acc did not improve from 0.99794\n",
      "Epoch 52/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4993 - acc: 0.9548 - val_loss: 0.4197 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00052: acc did not improve from 0.99794\n",
      "Epoch 53/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4940 - acc: 0.9555 - val_loss: 0.4162 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00053: acc did not improve from 0.99794\n",
      "Epoch 54/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.4739 - acc: 0.9592 - val_loss: 0.4135 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00054: acc did not improve from 0.99794\n",
      "Epoch 55/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.4772 - acc: 0.9587 - val_loss: 0.4255 - val_acc: 0.9669\n",
      "\n",
      "Epoch 00055: acc did not improve from 0.99794\n",
      "Epoch 56/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4657 - acc: 0.9608 - val_loss: 0.3968 - val_acc: 0.9719\n",
      "\n",
      "Epoch 00056: acc did not improve from 0.99794\n",
      "Epoch 57/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4677 - acc: 0.9603 - val_loss: 0.3979 - val_acc: 0.9714\n",
      "\n",
      "Epoch 00057: acc did not improve from 0.99794\n",
      "Epoch 58/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4586 - acc: 0.9621 - val_loss: 0.3979 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00058: acc did not improve from 0.99794\n",
      "Epoch 59/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4527 - acc: 0.9628 - val_loss: 0.4005 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00059: acc did not improve from 0.99794\n",
      "Epoch 60/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.4718 - acc: 0.9597 - val_loss: 0.3928 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00060: acc did not improve from 0.99794\n",
      "Epoch 61/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4558 - acc: 0.9624 - val_loss: 0.4005 - val_acc: 0.9709\n",
      "\n",
      "Epoch 00061: acc did not improve from 0.99794\n",
      "Epoch 62/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.4664 - acc: 0.9605 - val_loss: 0.3899 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00062: acc did not improve from 0.99794\n",
      "Epoch 63/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.4458 - acc: 0.9642 - val_loss: 0.3925 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00063: acc did not improve from 0.99794\n",
      "Epoch 64/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4469 - acc: 0.9637 - val_loss: 0.3897 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00064: acc did not improve from 0.99794\n",
      "Epoch 65/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4543 - acc: 0.9627 - val_loss: 0.3862 - val_acc: 0.9735\n",
      "\n",
      "Epoch 00065: acc did not improve from 0.99794\n",
      "Epoch 66/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4462 - acc: 0.9638 - val_loss: 0.3849 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00066: acc did not improve from 0.99794\n",
      "Epoch 67/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4480 - acc: 0.9639 - val_loss: 0.3783 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00067: acc did not improve from 0.99794\n",
      "Epoch 68/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4423 - acc: 0.9644 - val_loss: 0.3885 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00068: acc did not improve from 0.99794\n",
      "Epoch 69/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4346 - acc: 0.9658 - val_loss: 0.3753 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00069: acc did not improve from 0.99794\n",
      "Epoch 70/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4383 - acc: 0.9652 - val_loss: 0.3748 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00070: acc did not improve from 0.99794\n",
      "Epoch 71/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4311 - acc: 0.9663 - val_loss: 0.3811 - val_acc: 0.9738\n",
      "\n",
      "Epoch 00071: acc did not improve from 0.99794\n",
      "Epoch 72/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.4264 - acc: 0.9670 - val_loss: 0.3711 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00072: acc did not improve from 0.99794\n",
      "Epoch 73/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.4143 - acc: 0.9688 - val_loss: 0.3698 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00073: acc did not improve from 0.99794\n",
      "Epoch 74/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.4149 - acc: 0.9688 - val_loss: 0.3630 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00074: acc did not improve from 0.99794\n",
      "Epoch 75/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4170 - acc: 0.9685 - val_loss: 0.3718 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00075: acc did not improve from 0.99794\n",
      "Epoch 76/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4179 - acc: 0.9682 - val_loss: 0.3735 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00076: acc did not improve from 0.99794\n",
      "Epoch 77/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4079 - acc: 0.9699 - val_loss: 0.3568 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00077: acc did not improve from 0.99794\n",
      "Epoch 78/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4117 - acc: 0.9695 - val_loss: 0.3549 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00078: acc did not improve from 0.99794\n",
      "Epoch 79/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4032 - acc: 0.9703 - val_loss: 0.3641 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00079: acc did not improve from 0.99794\n",
      "Epoch 80/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.4053 - acc: 0.9701 - val_loss: 0.3701 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00080: acc did not improve from 0.99794\n",
      "Epoch 81/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.3932 - acc: 0.9720 - val_loss: 0.3491 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00081: acc did not improve from 0.99794\n",
      "Epoch 82/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.3955 - acc: 0.9716 - val_loss: 0.3505 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00082: acc did not improve from 0.99794\n",
      "Epoch 83/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.3973 - acc: 0.9713 - val_loss: 0.3427 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00083: acc did not improve from 0.99794\n",
      "Epoch 84/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.3938 - acc: 0.9717 - val_loss: 0.3502 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00084: acc did not improve from 0.99794\n",
      "Epoch 85/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3918 - acc: 0.9720 - val_loss: 0.3429 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00085: acc did not improve from 0.99794\n",
      "Epoch 86/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3932 - acc: 0.9719 - val_loss: 0.3400 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00086: acc did not improve from 0.99794\n",
      "Epoch 87/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3723 - acc: 0.9746 - val_loss: 0.3396 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00087: acc did not improve from 0.99794\n",
      "Epoch 88/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3912 - acc: 0.9721 - val_loss: 0.3474 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00088: acc did not improve from 0.99794\n",
      "Epoch 89/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3769 - acc: 0.9742 - val_loss: 0.3459 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00089: acc did not improve from 0.99794\n",
      "Epoch 90/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3845 - acc: 0.9732 - val_loss: 0.3383 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00090: acc did not improve from 0.99794\n",
      "Epoch 91/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.3745 - acc: 0.9744 - val_loss: 0.3425 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00091: acc did not improve from 0.99794\n",
      "Epoch 92/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3624 - acc: 0.9760 - val_loss: 0.3307 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00092: acc did not improve from 0.99794\n",
      "Epoch 93/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3681 - acc: 0.9753 - val_loss: 0.3361 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00093: acc did not improve from 0.99794\n",
      "Epoch 94/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3624 - acc: 0.9760 - val_loss: 0.3275 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00094: acc did not improve from 0.99794\n",
      "Epoch 95/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3734 - acc: 0.9745 - val_loss: 0.3279 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00095: acc did not improve from 0.99794\n",
      "Epoch 96/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.3551 - acc: 0.9770 - val_loss: 0.3307 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00096: acc did not improve from 0.99794\n",
      "Epoch 97/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3684 - acc: 0.9751 - val_loss: 0.3213 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00097: acc did not improve from 0.99794\n",
      "Epoch 98/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3496 - acc: 0.9776 - val_loss: 0.3411 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00098: acc did not improve from 0.99794\n",
      "Epoch 99/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3688 - acc: 0.9752 - val_loss: 0.3167 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00099: acc did not improve from 0.99794\n",
      "Epoch 100/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3481 - acc: 0.9779 - val_loss: 0.3297 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00100: acc did not improve from 0.99794\n",
      "Epoch 101/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3442 - acc: 0.9782 - val_loss: 0.3316 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00101: acc did not improve from 0.99794\n",
      "Epoch 102/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3538 - acc: 0.9772 - val_loss: 0.3201 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00102: acc did not improve from 0.99794\n",
      "Epoch 103/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3382 - acc: 0.9792 - val_loss: 0.3199 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00103: acc did not improve from 0.99794\n",
      "Epoch 104/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3456 - acc: 0.9780 - val_loss: 0.3208 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00104: acc did not improve from 0.99794\n",
      "Epoch 105/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3471 - acc: 0.9780 - val_loss: 0.3232 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00105: acc did not improve from 0.99794\n",
      "Epoch 106/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3446 - acc: 0.9783 - val_loss: 0.3238 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00106: acc did not improve from 0.99794\n",
      "Epoch 107/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3431 - acc: 0.9783 - val_loss: 0.3236 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00107: acc did not improve from 0.99794\n",
      "Epoch 108/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.3332 - acc: 0.9795 - val_loss: 0.3120 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00108: acc did not improve from 0.99794\n",
      "Epoch 109/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3389 - acc: 0.9790 - val_loss: 0.3154 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00109: acc did not improve from 0.99794\n",
      "Epoch 110/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3286 - acc: 0.9800 - val_loss: 0.3139 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00110: acc did not improve from 0.99794\n",
      "Epoch 111/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3305 - acc: 0.9798 - val_loss: 0.3158 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00111: acc did not improve from 0.99794\n",
      "Epoch 112/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3207 - acc: 0.9809 - val_loss: 0.3179 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00112: acc did not improve from 0.99794\n",
      "Epoch 113/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3227 - acc: 0.9808 - val_loss: 0.3161 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00113: acc did not improve from 0.99794\n",
      "Epoch 114/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3256 - acc: 0.9804 - val_loss: 0.3082 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00114: acc did not improve from 0.99794\n",
      "Epoch 115/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3127 - acc: 0.9819 - val_loss: 0.3055 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00115: acc did not improve from 0.99794\n",
      "Epoch 116/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.3241 - acc: 0.9806 - val_loss: 0.3227 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00116: acc did not improve from 0.99794\n",
      "Epoch 117/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3203 - acc: 0.9811 - val_loss: 0.3112 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00117: acc did not improve from 0.99794\n",
      "Epoch 118/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3201 - acc: 0.9811 - val_loss: 0.3020 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00118: acc did not improve from 0.99794\n",
      "Epoch 119/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3258 - acc: 0.9804 - val_loss: 0.2973 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00119: acc did not improve from 0.99794\n",
      "Epoch 120/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.3067 - acc: 0.9826 - val_loss: 0.3058 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00120: acc did not improve from 0.99794\n",
      "Epoch 121/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3160 - acc: 0.9815 - val_loss: 0.3155 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00121: acc did not improve from 0.99794\n",
      "Epoch 122/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3138 - acc: 0.9816 - val_loss: 0.3139 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00122: acc did not improve from 0.99794\n",
      "Epoch 123/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2962 - acc: 0.9836 - val_loss: 0.3105 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00123: acc did not improve from 0.99794\n",
      "Epoch 124/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3051 - acc: 0.9827 - val_loss: 0.3098 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00124: acc did not improve from 0.99794\n",
      "Epoch 125/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2999 - acc: 0.9831 - val_loss: 0.2999 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00125: acc did not improve from 0.99794\n",
      "Epoch 126/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2929 - acc: 0.9842 - val_loss: 0.3028 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00126: acc did not improve from 0.99794\n",
      "Epoch 127/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3070 - acc: 0.9823 - val_loss: 0.3016 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00127: acc did not improve from 0.99794\n",
      "Epoch 128/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2983 - acc: 0.9834 - val_loss: 0.2996 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00128: acc did not improve from 0.99794\n",
      "Epoch 129/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.2951 - acc: 0.9836 - val_loss: 0.2971 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00129: acc did not improve from 0.99794\n",
      "Epoch 130/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2879 - acc: 0.9846 - val_loss: 0.3015 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00130: acc did not improve from 0.99794\n",
      "Epoch 131/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2956 - acc: 0.9836 - val_loss: 0.2983 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00131: acc did not improve from 0.99794\n",
      "Epoch 132/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.3060 - acc: 0.9824 - val_loss: 0.2902 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00132: acc did not improve from 0.99794\n",
      "Epoch 133/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2920 - acc: 0.9842 - val_loss: 0.2956 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00133: acc did not improve from 0.99794\n",
      "Epoch 134/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2763 - acc: 0.9858 - val_loss: 0.3013 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00134: acc did not improve from 0.99794\n",
      "Epoch 135/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2915 - acc: 0.9840 - val_loss: 0.3141 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00135: acc did not improve from 0.99794\n",
      "Epoch 136/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2861 - acc: 0.9846 - val_loss: 0.2898 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00136: acc did not improve from 0.99794\n",
      "Epoch 137/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2832 - acc: 0.9851 - val_loss: 0.2948 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00137: acc did not improve from 0.99794\n",
      "Epoch 138/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2948 - acc: 0.9836 - val_loss: 0.3005 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00138: acc did not improve from 0.99794\n",
      "Epoch 139/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2756 - acc: 0.9857 - val_loss: 0.2951 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00139: acc did not improve from 0.99794\n",
      "Epoch 140/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2841 - acc: 0.9847 - val_loss: 0.3004 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00140: acc did not improve from 0.99794\n",
      "Epoch 141/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2776 - acc: 0.9855 - val_loss: 0.2882 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00141: acc did not improve from 0.99794\n",
      "Epoch 142/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2739 - acc: 0.9859 - val_loss: 0.2840 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00142: acc did not improve from 0.99794\n",
      "Epoch 143/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2730 - acc: 0.9861 - val_loss: 0.2883 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00143: acc did not improve from 0.99794\n",
      "Epoch 144/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2748 - acc: 0.9858 - val_loss: 0.2873 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00144: acc did not improve from 0.99794\n",
      "Epoch 145/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2729 - acc: 0.9858 - val_loss: 0.2929 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00145: acc did not improve from 0.99794\n",
      "Epoch 146/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2749 - acc: 0.9858 - val_loss: 0.3012 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00146: acc did not improve from 0.99794\n",
      "Epoch 147/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2714 - acc: 0.9862 - val_loss: 0.3038 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00147: acc did not improve from 0.99794\n",
      "Epoch 148/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2635 - acc: 0.9871 - val_loss: 0.3056 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00148: acc did not improve from 0.99794\n",
      "Epoch 149/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2703 - acc: 0.9860 - val_loss: 0.3059 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00149: acc did not improve from 0.99794\n",
      "Epoch 150/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2680 - acc: 0.9865 - val_loss: 0.2931 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00150: acc did not improve from 0.99794\n",
      "Epoch 151/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2658 - acc: 0.9865 - val_loss: 0.2802 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00151: acc did not improve from 0.99794\n",
      "Epoch 152/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2563 - acc: 0.9876 - val_loss: 0.2809 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00152: acc did not improve from 0.99794\n",
      "Epoch 153/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2612 - acc: 0.9871 - val_loss: 0.2927 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00153: acc did not improve from 0.99794\n",
      "Epoch 154/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2565 - acc: 0.9876 - val_loss: 0.2798 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00154: acc did not improve from 0.99794\n",
      "Epoch 155/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2671 - acc: 0.9864 - val_loss: 0.2926 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00155: acc did not improve from 0.99794\n",
      "Epoch 156/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2582 - acc: 0.9873 - val_loss: 0.2896 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00156: acc did not improve from 0.99794\n",
      "Epoch 157/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2470 - acc: 0.9885 - val_loss: 0.2864 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00157: acc did not improve from 0.99794\n",
      "Epoch 158/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2526 - acc: 0.9878 - val_loss: 0.2904 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00158: acc did not improve from 0.99794\n",
      "Epoch 159/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2579 - acc: 0.9874 - val_loss: 0.2911 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00159: acc did not improve from 0.99794\n",
      "Epoch 160/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2588 - acc: 0.9873 - val_loss: 0.2844 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00160: acc did not improve from 0.99794\n",
      "Epoch 161/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2479 - acc: 0.9881 - val_loss: 0.2856 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00161: acc did not improve from 0.99794\n",
      "Epoch 162/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.2464 - acc: 0.9885 - val_loss: 0.2813 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00162: acc did not improve from 0.99794\n",
      "Epoch 163/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2465 - acc: 0.9882 - val_loss: 0.2835 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00163: acc did not improve from 0.99794\n",
      "Epoch 164/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2468 - acc: 0.9885 - val_loss: 0.2929 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00164: acc did not improve from 0.99794\n",
      "Epoch 165/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2455 - acc: 0.9886 - val_loss: 0.2971 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00165: acc did not improve from 0.99794\n",
      "Epoch 166/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2550 - acc: 0.9872 - val_loss: 0.2893 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00166: acc did not improve from 0.99794\n",
      "Epoch 167/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2381 - acc: 0.9893 - val_loss: 0.2824 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00167: acc did not improve from 0.99794\n",
      "Epoch 168/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2452 - acc: 0.9887 - val_loss: 0.2812 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00168: acc did not improve from 0.99794\n",
      "Epoch 169/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2457 - acc: 0.9886 - val_loss: 0.2885 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00169: acc did not improve from 0.99794\n",
      "Epoch 170/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2496 - acc: 0.9882 - val_loss: 0.2736 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00170: acc did not improve from 0.99794\n",
      "Epoch 171/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2432 - acc: 0.9884 - val_loss: 0.2859 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00171: acc did not improve from 0.99794\n",
      "Epoch 172/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2431 - acc: 0.9888 - val_loss: 0.2797 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00172: acc did not improve from 0.99794\n",
      "Epoch 173/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2414 - acc: 0.9889 - val_loss: 0.2760 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00173: acc did not improve from 0.99794\n",
      "Epoch 174/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2282 - acc: 0.9902 - val_loss: 0.2782 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00174: acc did not improve from 0.99794\n",
      "Epoch 175/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2311 - acc: 0.9897 - val_loss: 0.2663 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00175: acc did not improve from 0.99794\n",
      "Epoch 176/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2359 - acc: 0.9893 - val_loss: 0.2817 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00176: acc did not improve from 0.99794\n",
      "Epoch 177/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2470 - acc: 0.9882 - val_loss: 0.2824 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00177: acc did not improve from 0.99794\n",
      "Epoch 178/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2269 - acc: 0.9900 - val_loss: 0.2698 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00178: acc did not improve from 0.99794\n",
      "Epoch 179/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2297 - acc: 0.9899 - val_loss: 0.2786 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00179: acc did not improve from 0.99794\n",
      "Epoch 180/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2325 - acc: 0.9895 - val_loss: 0.2785 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00180: acc did not improve from 0.99794\n",
      "Epoch 181/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2193 - acc: 0.9908 - val_loss: 0.2740 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00181: acc did not improve from 0.99794\n",
      "Epoch 182/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2235 - acc: 0.9905 - val_loss: 0.2708 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00182: acc did not improve from 0.99794\n",
      "Epoch 183/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2168 - acc: 0.9911 - val_loss: 0.2765 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00183: acc did not improve from 0.99794\n",
      "Epoch 184/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2234 - acc: 0.9903 - val_loss: 0.2933 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00184: acc did not improve from 0.99794\n",
      "Epoch 185/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2230 - acc: 0.9904 - val_loss: 0.2814 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00185: acc did not improve from 0.99794\n",
      "Epoch 186/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2250 - acc: 0.9902 - val_loss: 0.2829 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00186: acc did not improve from 0.99794\n",
      "Epoch 187/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2201 - acc: 0.9904 - val_loss: 0.2783 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00187: acc did not improve from 0.99794\n",
      "Epoch 188/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2317 - acc: 0.9894 - val_loss: 0.2824 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00188: acc did not improve from 0.99794\n",
      "Epoch 189/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2205 - acc: 0.9904 - val_loss: 0.2827 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00189: acc did not improve from 0.99794\n",
      "Epoch 190/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2237 - acc: 0.9905 - val_loss: 0.2851 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00190: acc did not improve from 0.99794\n",
      "Epoch 191/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2151 - acc: 0.9912 - val_loss: 0.2949 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00191: acc did not improve from 0.99794\n",
      "Epoch 192/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2194 - acc: 0.9908 - val_loss: 0.2832 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00192: acc did not improve from 0.99794\n",
      "Epoch 193/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2161 - acc: 0.9908 - val_loss: 0.2831 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00193: acc did not improve from 0.99794\n",
      "Epoch 194/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2123 - acc: 0.9911 - val_loss: 0.2728 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00194: acc did not improve from 0.99794\n",
      "Epoch 195/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.2130 - acc: 0.9913 - val_loss: 0.2806 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00195: acc did not improve from 0.99794\n",
      "Epoch 196/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2245 - acc: 0.9901 - val_loss: 0.2770 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00196: acc did not improve from 0.99794\n",
      "Epoch 197/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2063 - acc: 0.9917 - val_loss: 0.2864 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00197: acc did not improve from 0.99794\n",
      "Epoch 198/200\n",
      "64/64 [==============================] - 24s 371ms/step - loss: 0.2134 - acc: 0.9911 - val_loss: 0.2819 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00198: acc did not improve from 0.99794\n",
      "Epoch 199/200\n",
      "64/64 [==============================] - 24s 370ms/step - loss: 0.2041 - acc: 0.9921 - val_loss: 0.2725 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00199: acc did not improve from 0.99794\n",
      "Epoch 200/200\n",
      "64/64 [==============================] - 24s 369ms/step - loss: 0.1991 - acc: 0.9922 - val_loss: 0.2807 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00200: acc did not improve from 0.99794\n"
     ]
    }
   ],
   "source": [
    "# Training the CNN model1\n",
    "print(\"Training the InceptionV3 model\")\n",
    "history1 = model1.fit(x_train, y_train_cat, batch_size=128, epochs=200, verbose=1, validation_data=(x_test, y_test_cat),callbacks=callback_list)\n",
    "\n",
    "print(\"Training the InceptionResNet model\")\n",
    "history2 = model2.fit(x_train, y_train_cat, batch_size = 128, epochs = 200, verbose = 1, validation_data = (x_test, y_test_cat), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction by all models\n"
     ]
    }
   ],
   "source": [
    "models = [model1, model2]\n",
    "print (\"prediction by all models\")\n",
    "preds = [model.predict(x_test) for model in models]\n",
    "preds=np.array(preds)\n",
    "\n",
    "summed = np.sum(preds, axis=0) #computes the sum of probability by all 2 models for each class\n",
    "\n",
    "ensemble_prediction = np.argmax(summed, axis=1) #predicts the class labels based on maximum probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting classes by individual models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noush\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing accuracy by all models\n",
      "Accuracy Score for model1 =  0.919140625\n",
      "Accuracy Score for model2 =  0.89296875\n",
      "ensemble accuracy = 0.94140625\n"
     ]
    }
   ],
   "source": [
    "print(\"predicting classes by individual models\")\n",
    "prediction1 = model1.predict_classes(x_test)\n",
    "prediction2 = model2.predict_classes(x_test)\n",
    "\n",
    "print(\"computing accuracy by all models\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy1 = accuracy_score(y_test, prediction1)\n",
    "accuracy2 = accuracy_score(y_test, prediction2)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_prediction)\n",
    "\n",
    "\n",
    "print('Accuracy Score for model1 = ', accuracy1)\n",
    "print('Accuracy Score for model2 = ', accuracy2)\n",
    "print('ensemble accuracy =', ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity or TNR 0.9804309598115863\n",
      "Sensitivity or TPR or Recall 0.9414373324539381\n",
      "FNR  0.058562667546061875\n",
      "FPR 0.019569040188413656\n",
      "Accuracy =  0.94140625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(58.222222222222214, 0.5, 'True Label')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "#%%\n",
    "\n",
    "\n",
    "mcm = multilabel_confusion_matrix(y_true=y_test, y_pred=ensemble_prediction, labels=[0,1,2,3], samplewise=False)\n",
    "tn = mcm[:,0,0]\n",
    "tp = mcm[:,1,1]\n",
    "fp = mcm[:,0,1]\n",
    "fn = mcm[:,1,0]\n",
    "specificity = tn/(tn+fp)\n",
    "print(\"Specificity or TNR\", np.mean(specificity))\n",
    "\n",
    "sensitivity = tp/(tp+fn)\n",
    "print(\"Sensitivity or TPR or Recall\", np.mean(sensitivity))\n",
    "\n",
    "print(\"FNR \", 1-np.mean(sensitivity))\n",
    "\n",
    "print(\"FPR\", 1-np.mean(specificity))\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy = \", metrics.accuracy_score(y_test, ensemble_prediction))\n",
    "#%%\n",
    "# # use model to predict probability that given y value is 1\n",
    "# y_pred_proba = resnet_model.predict_proba(x_test)\n",
    "\n",
    "# #calculate AUC of model\n",
    "# auc = metrics.roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "\n",
    "# #print AUC score\n",
    "# print(\"AUC =\",auc)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cf_matrix = confusion_matrix(y_test, ensemble_prediction)\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(7, 6))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='Blues', cbar=False, linewidth=0.5,linecolor=\"black\",fmt='')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
